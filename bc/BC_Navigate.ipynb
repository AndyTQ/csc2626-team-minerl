{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pxwl_yG1qhR7"
      },
      "source": [
        "<div style=\"text-align: center\">\n",
        "  <img src=\"https://github.com/KarolisRam/MineRL2021-Intro-baselines/blob/main/img/colab_banner.png?raw=true\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_BGlQwngccr"
      },
      "source": [
        "# Introduction\n",
        "This notebook is part three of the Intro track baselines for the [MineRL 2021](https://minerl.io/) competition. To run it you will need to enable GPU by going to `Runtime -> Change runtime type` and selecting GPU from the drop down list.\n",
        "\n",
        "Below you will find an agent that has two components:\n",
        "1. A machine learning agent that trains on human data to learn how to imitate them to chop trees (training takes less than 10 minutes).\n",
        "2. A script that crafts a wooden pickaxe and digs down to get some cobblestone.  \n",
        "\n",
        "The machine learning part runs for a fixed number of steps (2000 by default), then the crafting and digging script kicks in.\n",
        "When evaluated on MineRLObtainDiamond environment it achieves an average reward of 8.6.\n",
        "\n",
        "## Software 2.0\n",
        "The approach we used here, where we took some human written code and replaced it with an AI component is quite similar to how Tesla approaches self driving cars. See this talk by Andrej Karpathy, Director of AI at Tesla:  \n",
        "[Building the Software 2.0 Stack](https://databricks.com/session/keynote-from-tesla)\n",
        "\n",
        "Go on, improve the self driving Steve/Alex below! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysSTXmT3YUeF"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3vdSzohL6vb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJsmuzsgYGCY"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HTScYNljgXv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!sudo add-apt-repository -y ppa:openjdk-r/ppa\n",
        "!sudo apt-get purge openjdk-*\n",
        "!sudo apt-get install openjdk-8-jdk\n",
        "!sudo apt-get install xvfb xserver-xephyr vnc4server python-opengl ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldm-wTgpOBYK"
      },
      "outputs": [],
      "source": [
        "!pip3 install minerl==0.4.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh6gb3UWjT3p"
      },
      "outputs": [],
      "source": [
        "# !pip3 install --upgrade minerl\n",
        "!pip3 install pyvirtualdisplay\n",
        "!pip3 install --verbose torch\n",
        "!pip3 install -U colabgymrender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt2EEUtZR9QO"
      },
      "outputs": [],
      "source": [
        "!pip3 install imageio-ffmpeg\n",
        "!pip3 install imageio==2.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37QpnBeXZyp5"
      },
      "outputs": [],
      "source": [
        "!pip3 install gym==0.19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADmrUKxvYXGa"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8_vZpMFpiD9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch as th\n",
        "from torch import nn\n",
        "import gym\n",
        "import minerl\n",
        "from tqdm.notebook import tqdm\n",
        "from colabgymrender.recorder import Recorder\n",
        "from pyvirtualdisplay import Display\n",
        "import logging\n",
        "logging.disable(logging.ERROR) # reduce clutter, remove if something doesn't work to see the error logs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKiasaipYa6l"
      },
      "source": [
        "# Neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyOxGuA5At1g"
      },
      "outputs": [],
      "source": [
        "class NatureCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN from DQN nature paper:\n",
        "        Mnih, Volodymyr, et al.\n",
        "        \"Human-level control through deep reinforcement learning.\"\n",
        "        Nature 518.7540 (2015): 529-533.\n",
        "\n",
        "    :param input_shape: A three-item tuple telling image dimensions in (C, H, W)\n",
        "    :param output_dim: Dimensionality of the output vector\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, output_dim):\n",
        "        super().__init__()\n",
        "        n_input_channels = input_shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Compute shape by doing one forward pass\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(th.zeros(1, *input_shape)).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(n_flatten, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        return self.linear(self.cnn(observations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WwX1vgpYfuC"
      },
      "source": [
        "# Environment wrappers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8em_oPbA9PQ"
      },
      "outputs": [],
      "source": [
        "class ActionShaping(gym.ActionWrapper):\n",
        "    \"\"\"\n",
        "    The default MineRL action space is the following dict:\n",
        "\n",
        "    Dict(attack:Discrete(2),\n",
        "         back:Discrete(2),\n",
        "         camera:Box(low=-180.0, high=180.0, shape=(2,)),\n",
        "         craft:Enum(crafting_table,none,planks,stick,torch),\n",
        "         equip:Enum(air,iron_axe,iron_pickaxe,none,stone_axe,stone_pickaxe,wooden_axe,wooden_pickaxe),\n",
        "         forward:Discrete(2),\n",
        "         jump:Discrete(2),\n",
        "         left:Discrete(2),\n",
        "         nearbyCraft:Enum(furnace,iron_axe,iron_pickaxe,none,stone_axe,stone_pickaxe,wooden_axe,wooden_pickaxe),\n",
        "         nearbySmelt:Enum(coal,iron_ingot,none),\n",
        "         place:Enum(cobblestone,crafting_table,dirt,furnace,none,stone,torch),\n",
        "         right:Discrete(2),\n",
        "         sneak:Discrete(2),\n",
        "         sprint:Discrete(2))\n",
        "\n",
        "    It can be viewed as:\n",
        "         - buttons, like attack, back, forward, sprint that are either pressed or not.\n",
        "         - mouse, i.e. the continuous camera action in degrees. The two values are pitch (up/down), where up is\n",
        "           negative, down is positive, and yaw (left/right), where left is negative, right is positive.\n",
        "         - craft/equip/place actions for items specified above.\n",
        "    So an example action could be sprint + forward + jump + attack + turn camera, all in one action.\n",
        "\n",
        "    This wrapper makes the action space much smaller by selecting a few common actions and making the camera actions\n",
        "    discrete. You can change these actions by changing self._actions below. That should just work with the RL agent,\n",
        "    but would require some further tinkering below with the BC one.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, camera_angle=10, always_attack=False):\n",
        "        super().__init__(env)\n",
        "\n",
        "        self.camera_angle = camera_angle\n",
        "        self.always_attack = always_attack\n",
        "        self._actions = [\n",
        "            [('attack', 1)],\n",
        "            [('forward', 1)],\n",
        "            # [('back', 1)],\n",
        "            # [('left', 1)],\n",
        "            # [('right', 1)],\n",
        "            # [('jump', 1)],\n",
        "            # [('forward', 1), ('attack', 1)],\n",
        "            # [('craft', 'planks')],\n",
        "            [('forward', 1), ('jump', 1)],\n",
        "            [('camera', [-self.camera_angle, 0])],\n",
        "            [('camera', [self.camera_angle, 0])],\n",
        "            [('camera', [0, self.camera_angle])],\n",
        "            [('camera', [0, -self.camera_angle])],\n",
        "        ]\n",
        "\n",
        "        self.actions = []\n",
        "        for actions in self._actions:\n",
        "            act = self.env.action_space.noop()\n",
        "            for a, v in actions:\n",
        "                act[a] = v\n",
        "            if self.always_attack:\n",
        "                act['attack'] = 1\n",
        "            self.actions.append(act)\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(len(self.actions))\n",
        "\n",
        "    def action(self, action):\n",
        "        return self.actions[action]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFI364GwY6Oe"
      },
      "source": [
        "# Data parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cZo-d6pA4br"
      },
      "outputs": [],
      "source": [
        "def dataset_action_batch_to_actions(dataset_actions, camera_margin=5):\n",
        "    \"\"\"\n",
        "    Turn a batch of actions from dataset (`batch_iter`) to a numpy\n",
        "    array that corresponds to batch of actions of ActionShaping wrapper (_actions).\n",
        "\n",
        "    Camera margin sets the threshold what is considered \"moving camera\".\n",
        "\n",
        "    Note: Hardcoded to work for actions in ActionShaping._actions, with \"intuitive\"\n",
        "        ordering of actions.\n",
        "        If you change ActionShaping._actions, remember to change this!\n",
        "\n",
        "    Array elements are integers corresponding to actions, or \"-1\"\n",
        "    for actions that did not have any corresponding discrete match.\n",
        "    \"\"\"\n",
        "    # There are dummy dimensions of shape one\n",
        "    camera_actions = dataset_actions[\"camera\"].squeeze()\n",
        "    attack_actions = dataset_actions[\"attack\"].squeeze()\n",
        "    forward_actions = dataset_actions[\"forward\"].squeeze()\n",
        "    jump_actions = dataset_actions[\"jump\"].squeeze()\n",
        "    batch_size = len(camera_actions)\n",
        "    actions = np.zeros((batch_size,), dtype=np.int)\n",
        "\n",
        "    for i in range(len(camera_actions)):\n",
        "        # Moving camera is most important (horizontal first)\n",
        "        if camera_actions[i][0] < -camera_margin:\n",
        "            actions[i] = 3\n",
        "        elif camera_actions[i][0] > camera_margin:\n",
        "            actions[i] = 4\n",
        "        elif camera_actions[i][1] > camera_margin:\n",
        "            actions[i] = 5\n",
        "        elif camera_actions[i][1] < -camera_margin:\n",
        "            actions[i] = 6\n",
        "        elif forward_actions[i] == 1:\n",
        "            if jump_actions[i] == 1:\n",
        "                actions[i] = 2\n",
        "            else:\n",
        "                actions[i] = 1\n",
        "        elif attack_actions[i] == 1:\n",
        "            actions[i] = 0\n",
        "        else:\n",
        "            # No reasonable mapping (would be no-op)\n",
        "            actions[i] = -1\n",
        "    return actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvrJks0gZCTW"
      },
      "source": [
        "# Setup training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpH8vzpLBGRY"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    data = minerl.data.make(\"MineRLNavigateDense-v0\",  data_dir='data', num_workers=4)\n",
        "\n",
        "    # We know ActionShaping has seven discrete actions, so we create\n",
        "    # a network to map images to seven values (logits), which represent\n",
        "    # likelihoods of selecting those actions\n",
        "    network = NatureCNN((3, 64, 64), 7).cuda()\n",
        "    optimizer = th.optim.Adam(network.parameters(), lr=LEARNING_RATE)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    iter_count = 0\n",
        "    losses = []\n",
        "    for dataset_obs, dataset_actions, _, _, _ in tqdm(data.batch_iter(num_epochs=EPOCHS, batch_size=32, seq_len=1)):\n",
        "        pov = dataset_obs['pov']\n",
        "        # Transpose observations to be channel-first (BCHW instead of BHWC)\n",
        "        obs = obs.transpose(0, 3, 1, 2)\n",
        "        # Normalize observations\n",
        "        obs /= 255.0\n",
        "\n",
        "        # Actions need bit more work\n",
        "        actions = dataset_action_batch_to_actions(dataset_actions)\n",
        "\n",
        "        # Remove samples that had no corresponding action\n",
        "        mask = actions != -1\n",
        "        obs = obs[mask]\n",
        "        actions = actions[mask]\n",
        "\n",
        "        # Obtain logits of each action\n",
        "        logits = network(th.from_numpy(obs).float().cuda())\n",
        "\n",
        "        # Minimize cross-entropy with target labels.\n",
        "        # We could also compute the probability of demonstration actions and\n",
        "        # maximize them.\n",
        "        loss = loss_function(logits, th.from_numpy(actions).long().cuda())\n",
        "\n",
        "        # Standard PyTorch update\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        iter_count += 1\n",
        "        losses.append(loss.item())\n",
        "        if (iter_count % 1000) == 0:\n",
        "            mean_loss = sum(losses) / len(losses)\n",
        "            tqdm.write(\"Iteration {}. Loss {:<10.3f}\".format(iter_count, mean_loss))\n",
        "            th.save(network.state_dict(), TRAIN_MODEL_NAME + \"-\" + str(iter_count))\n",
        "            losses.clear()\n",
        "\n",
        "    th.save(network.state_dict(), TRAIN_MODEL_NAME + \"-final\")\n",
        "    del data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl1w4hAdZL2y"
      },
      "source": [
        "# Scripted part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZDmSMRP8eto"
      },
      "outputs": [],
      "source": [
        "def str_to_act(env, actions):\n",
        "    \"\"\"\n",
        "    Simplifies specifying actions for the scripted part of the agent.\n",
        "    Some examples for a string with a single action:\n",
        "        'craft:planks'\n",
        "        'camera:[10,0]'\n",
        "        'attack'\n",
        "        'jump'\n",
        "        ''\n",
        "    There should be no spaces in single actions, as we use spaces to separate actions with multiple \"buttons\" pressed:\n",
        "        'attack sprint forward'\n",
        "        'forward camera:[0,10]'\n",
        "\n",
        "    :param env: base MineRL environment.\n",
        "    :param actions: string of actions.\n",
        "    :return: dict action, compatible with the base MineRL environment.\n",
        "    \"\"\"\n",
        "    act = env.action_space.noop()\n",
        "    for action in actions.split():\n",
        "        if \":\" in action:\n",
        "            k, v = action.split(':')\n",
        "            if k == 'camera':\n",
        "                act[k] = eval(v)\n",
        "            else:\n",
        "                act[k] = v\n",
        "        else:\n",
        "            act[action] = 1\n",
        "    return act"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaUKtO-3oVKu"
      },
      "source": [
        "# Actions\n",
        "Here's a list of all possible actions:\n",
        "```\n",
        "Dict(attack:Discrete(2),\n",
        "     back:Discrete(2),\n",
        "     camera:Box(low=-180.0, high=180.0, shape=(2,)),\n",
        "     craft:Enum(crafting_table,none,planks,stick,torch),\n",
        "     equip:Enum(air,iron_axe,iron_pickaxe,none,stone_axe,stone_pickaxe,wooden_axe,wooden_pickaxe),\n",
        "     forward:Discrete(2),\n",
        "     jump:Discrete(2),\n",
        "     left:Discrete(2),\n",
        "     nearbyCraft:Enum(furnace,iron_axe,iron_pickaxe,none,stone_axe,stone_pickaxe,wooden_axe,wooden_pickaxe),\n",
        "     nearbySmelt:Enum(coal,iron_ingot,none),\n",
        "     place:Enum(cobblestone,crafting_table,dirt,furnace,none,stone,torch),\n",
        "     right:Discrete(2),\n",
        "     sneak:Discrete(2),\n",
        "     sprint:Discrete(2))\n",
        "```\n",
        "\n",
        "### Camera\n",
        "Camera actions contain two values:\n",
        "1. Pitch (up/down), where up is negative, down is positive.\n",
        "2. Yaw (left/right), where left is negative, right is positive.  \n",
        "\n",
        "For example, moving the camera up by 10 degrees would be 'camera:[-10,0]'.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BzwD0sI8fbg"
      },
      "outputs": [],
      "source": [
        "def get_action_sequence():\n",
        "    \"\"\"\n",
        "    Specify the action sequence for the agent to execute.\n",
        "    \"\"\"\n",
        "    # make planks, sticks, crafting table and wooden pickaxe:\n",
        "    action_sequence = []\n",
        "    action_sequence += [''] * 100\n",
        "    action_sequence += ['craft:planks'] * 4\n",
        "    action_sequence += ['craft:stick'] * 2\n",
        "    action_sequence += ['craft:crafting_table']\n",
        "    action_sequence += ['camera:[10,0]'] * 18\n",
        "    action_sequence += ['attack'] * 20\n",
        "    action_sequence += [''] * 10\n",
        "    action_sequence += ['jump']\n",
        "    action_sequence += [''] * 5\n",
        "    action_sequence += ['place:crafting_table']\n",
        "    action_sequence += [''] * 10\n",
        "\n",
        "    # bug: looking straight down at a crafting table doesn't let you craft. So we look up a bit before crafting.\n",
        "    action_sequence += ['camera:[-1,0]']\n",
        "    action_sequence += ['nearbyCraft:wooden_pickaxe']\n",
        "    action_sequence += ['camera:[1,0]']\n",
        "    action_sequence += [''] * 10\n",
        "    action_sequence += ['equip:wooden_pickaxe']\n",
        "    action_sequence += [''] * 10\n",
        "\n",
        "    # dig down:\n",
        "    action_sequence += ['attack'] * 600\n",
        "    action_sequence += [''] * 10\n",
        "\n",
        "    return action_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg68dO21ZsgG"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5VCVeHyqDlm"
      },
      "outputs": [],
      "source": [
        "# Parameters:\n",
        "EPOCHS = 1  # How many times we train over the dataset. # 1 epochs = 12000 steps\n",
        "LEARNING_RATE = 0.0001  # Learning rate for the neural network.\n",
        "\n",
        "TRAIN_MODEL_NAME = 'another_potato.pth'  # name to use when saving the trained agent.\n",
        "TEST_MODEL_NAME = 'another_potato.pth'  # name to use when loading the trained agent.\n",
        "\n",
        "### Evaluation params\n",
        "TEST_EPISODES = 10  # number of episodes to test the agent for.\n",
        "MAX_TEST_EPISODE_LEN = 18000  # 18k is the default for MineRLObtainDiamond.\n",
        "TREECHOP_STEPS = 18000  # number of steps to run BC lumberjack for in evaluations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lumAopy0cHBM"
      },
      "source": [
        "# Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "NzD13IclpD4T",
        "outputId": "cea1863f-ef39-47d4-81e6-7ac13697665d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Download: https://minerl.s3.amazonaws.com/v4/MineRLNavigateDense-v0.tar: 100%|██████████| 634.0/633.56928 [00:09<00:00, 65.15MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "minerl.data.download(directory='data', environment='MineRLNavigateDense-v0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zKLHW_JcRBJ"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5YO0STEyuyk"
      },
      "outputs": [],
      "source": [
        "# !pip3 install --user --upgrade git+http://github.com/pyglet/pyglet@pyglet-1.5-maintenance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9OKQCgz4XQk"
      },
      "outputs": [],
      "source": [
        "display = Display(visible=0, size=(400, 300), backend=\"xvfb\")\n",
        "display.start();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_6j2-ibcxZh"
      },
      "source": [
        "# Start Minecraft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKE1nDWRs_7C",
        "outputId": "24687954-abcc-4c7a-9f7d-444ea9c05405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"1.8.0_352\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_352-8u352-ga-1~18.04-b08)\n",
            "OpenJDK 64-Bit Server VM (build 25.352-b08, mixed mode)\n"
          ]
        }
      ],
      "source": [
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "!java -version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja8WN5rgc3Ex"
      },
      "outputs": [],
      "source": [
        "env = gym.make('MineRLNavigateDense-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itdK92wrM3fo"
      },
      "outputs": [],
      "source": [
        "# Launch virtual display, which is needed for MineRL\n",
        "env1 = Recorder(env, './video', fps=60)  # saving environment before action shaping to use with scripted part\n",
        "env = ActionShaping(env1, always_attack=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH84zVpiB19e"
      },
      "outputs": [],
      "source": [
        "train()  # only need to run this once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjKpJgZicn6L"
      },
      "source": [
        "# Run your agent\n",
        "As the code below runs you should see episode videos and rewards show up. You can run the below cell multiple times to see different episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NcG2xpzOfAC"
      },
      "outputs": [],
      "source": [
        "network = NatureCNN((3, 64, 64), 7).cuda()\n",
        "network.load_state_dict(th.load(TEST_MODEL_NAME + \"-final\"))\n",
        "\n",
        "num_actions = env.action_space.n\n",
        "action_list = np.arange(num_actions)\n",
        "\n",
        "action_sequence = get_action_sequence()\n",
        "\n",
        "\n",
        "\n",
        "for episode in range(TEST_EPISODES):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "\n",
        "    # BC part to get some logs:\n",
        "    for i in tqdm(range(TREECHOP_STEPS)):\n",
        "        # Process the action:\n",
        "        #   - Add/remove batch dimensions\n",
        "        #   - Transpose image (needs to be channels-last)\n",
        "        #   - Normalize image\n",
        "        obs = th.from_numpy(obs['pov'].transpose(2, 0, 1)[None].astype(np.float32) / 255).cuda()\n",
        "        # Turn logits into probabilities\n",
        "        probabilities = th.softmax(network(obs), dim=1)[0]\n",
        "        # Into numpy\n",
        "        probabilities = probabilities.detach().cpu().numpy()\n",
        "        # Sample action according to the probabilities\n",
        "        action = np.random.choice(action_list, p=probabilities)\n",
        "\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # scripted part to use the logs:\n",
        "    # if not done:\n",
        "    #     for i, action in enumerate(tqdm(action_sequence[:MAX_TEST_EPISODE_LEN - TREECHOP_STEPS])):\n",
        "    #         obs, reward, done, _ = env1.step(str_to_act(env1, action))\n",
        "    #         total_reward += reward\n",
        "    #         steps += 1\n",
        "    #         if done:\n",
        "    #             break\n",
        "\n",
        "    env1.release()\n",
        "    env1.play()\n",
        "    print(f'Episode #{episode + 1} reward: {total_reward}\\t\\t episode length: {steps}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICzp7XgShQtv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vCi1S2vUjAq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}